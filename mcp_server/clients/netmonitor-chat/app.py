#!/usr/bin/env python3
# SPDX-License-Identifier: AGPL-3.0-only
# Copyright (c) 2025 Willem M. Poort
"""
NetMonitor Chat - Custom Web Interface

Simple, reliable web interface for Ollama + NetMonitor MCP Tools.
Built after discovering Open-WebUI doesn't support Streamable HTTP MCP servers.

Features:
- Clean chat interface
- Ollama integration (any model)
- Automatic tool calling via mcp_bridge.py
- Real-time streaming
- 100% on-premise
"""

import os
import sys
import json
import asyncio
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, AsyncGenerator
from datetime import datetime
from contextlib import asynccontextmanager

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse
from pydantic import BaseModel
from dotenv import load_dotenv
import httpx
import uvicorn

# Load .env file
load_dotenv()

# Add parent directory to path for mcp_bridge import
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Configuration
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "https://soc.poort.net/mcp")
MCP_AUTH_TOKEN = os.getenv("MCP_AUTH_TOKEN", "")

# MCP Bridge path - configurable for different deployments
_mcp_bridge_env = os.getenv("MCP_BRIDGE_PATH")
if _mcp_bridge_env:
    MCP_BRIDGE_PATH = Path(_mcp_bridge_env)
else:
    # Default: relative to this file (Linux server layout)
    MCP_BRIDGE_PATH = Path(__file__).parent.parent / "ollama-mcp-bridge" / "mcp_bridge.py"


# Pydantic models for request bodies
class LLMConfig(BaseModel):
    provider: str = "ollama"
    url: str = "http://localhost:11434"

class MCPConfig(BaseModel):
    url: str = "https://soc.poort.net/mcp"
    token: str = ""

class HealthConfig(BaseModel):
    llm_provider: str = "ollama"
    llm_url: str = "http://localhost:11434"
    mcp_url: str = "https://soc.poort.net/mcp"
    mcp_token: str = ""


# Lifespan event handler (replaces on_event)
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    yield
    # Shutdown - nothing to clean up as clients are created per-request now
    pass


# Initialize FastAPI
app = FastAPI(
    title="NetMonitor Chat",
    description="On-premise chat interface with NetMonitor MCP tools",
    version="1.0.0",
    lifespan=lifespan
)

# Mount static files
app.mount("/static", StaticFiles(directory=Path(__file__).parent / "static"), name="static")


class OllamaClient:
    """Client for Ollama API"""

    def __init__(self, base_url: str = OLLAMA_BASE_URL):
        self.base_url = base_url.rstrip('/')
        self.client = httpx.AsyncClient(timeout=120.0)

    async def list_models(self) -> List[Dict]:
        """List available Ollama models"""
        try:
            print(f"[Ollama] Fetching models from {self.base_url}/api/tags")
            response = await self.client.get(f"{self.base_url}/api/tags")
            response.raise_for_status()
            data = response.json()
            models = data.get("models", [])
            print(f"[Ollama] Found {len(models)} models")
            return models
        except httpx.ConnectError as e:
            print(f"[Ollama] Connection failed - is Ollama running on {self.base_url}? Error: {e}")
            return []
        except Exception as e:
            print(f"[Ollama] Error listing models: {type(e).__name__}: {e}")
            return []

    async def chat(
        self,
        model: str,
        messages: List[Dict],
        stream: bool = True,
        temperature: float = 0.7,
        tools: Optional[List[Dict]] = None
    ) -> AsyncGenerator:
        """
        Chat with Ollama model

        Args:
            model: Model name
            messages: Chat messages
            stream: Stream responses
            temperature: Sampling temperature
            tools: Available tools (for native tool calling)

        Yields:
            Response chunks
        """
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            "options": {
                "temperature": temperature
            }
        }

        if tools:
            payload["tools"] = tools

        try:
            async with self.client.stream(
                "POST",
                f"{self.base_url}/api/chat",
                json=payload
            ) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if line.strip():
                        try:
                            chunk = json.loads(line)
                            yield chunk
                        except json.JSONDecodeError:
                            continue
        except Exception as e:
            print(f"Error in chat: {e}")
            yield {"error": str(e)}

    async def close(self):
        """Close HTTP client"""
        await self.client.aclose()


class LMStudioClient:
    """Client for LM Studio (OpenAI-compatible API)"""

    def __init__(self, base_url: str = "http://localhost:1234"):
        self.base_url = base_url.rstrip('/')
        # Longer timeout for LM Studio (models can be slow to start generating)
        self.client = httpx.AsyncClient(timeout=300.0)

    async def list_models(self) -> List[Dict]:
        """List available LM Studio models"""
        try:
            print(f"[LM Studio] Fetching models from {self.base_url}/v1/models")
            response = await self.client.get(f"{self.base_url}/v1/models")
            response.raise_for_status()
            data = response.json()
            models = data.get("data", [])
            print(f"[LM Studio] Found {len(models)} models")
            # Convert OpenAI format to Ollama format for consistency
            return [{"name": m.get("id", "unknown")} for m in models]
        except httpx.ConnectError as e:
            print(f"[LM Studio] Connection failed - is LM Studio running on {self.base_url}? Error: {e}")
            return []
        except Exception as e:
            print(f"[LM Studio] Error listing models: {type(e).__name__}: {e}")
            return []

    async def chat(
        self,
        model: str,
        messages: List[Dict],
        stream: bool = True,
        temperature: float = 0.7,
        tools: Optional[List[Dict]] = None
    ) -> AsyncGenerator:
        """
        Chat with LM Studio model using OpenAI-compatible API

        Args:
            model: Model name
            messages: Chat messages
            stream: Stream responses
            temperature: Sampling temperature
            tools: Available tools (for function calling)

        Yields:
            Response chunks in Ollama format for compatibility
        """
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            "temperature": temperature
        }

        # Note: Most LM Studio models don't support function calling
        # Only add tools if explicitly provided, and handle errors gracefully
        # if tools:
        #     payload["tools"] = tools

        try:
            print(f"[LM Studio] Sending request to {self.base_url}/v1/chat/completions")
            print(f"[LM Studio] Model: {model}, Messages: {len(messages)}, Temperature: {temperature}")

            async with self.client.stream(
                "POST",
                f"{self.base_url}/v1/chat/completions",
                json=payload
            ) as response:
                print(f"[LM Studio] Response status: {response.status_code}")

                # Better error logging
                if response.status_code != 200:
                    error_text = await response.aread()
                    print(f"[LM Studio] Error {response.status_code}: {error_text.decode()}")
                    yield {"error": f"LM Studio error {response.status_code}: {error_text.decode()}"}
                    return

                response.raise_for_status()

                print("[LM Studio] Starting to read stream...")
                line_count = 0
                async for line in response.aiter_lines():
                    line_count += 1
                    if line_count <= 3:  # Log first 3 lines for debugging
                        print(f"[LM Studio] Line {line_count}: {line[:100] if line else 'empty'}")
                    if line.strip():
                        # Remove "data: " prefix if present
                        if line.startswith("data: "):
                            line = line[6:]

                        if line.strip() == "[DONE]":
                            yield {"done": True}
                            continue

                        try:
                            chunk = json.loads(line)
                            # Convert OpenAI format to Ollama format
                            choices = chunk.get("choices", [])
                            if choices:
                                delta = choices[0].get("delta", {})
                                content = delta.get("content", "")
                                tool_calls = delta.get("tool_calls", [])

                                # Build Ollama-compatible response
                                ollama_chunk = {
                                    "message": {
                                        "role": delta.get("role", "assistant"),
                                        "content": content
                                    },
                                    "done": choices[0].get("finish_reason") is not None
                                }

                                # Add tool calls if present
                                if tool_calls:
                                    ollama_chunk["message"]["tool_calls"] = [
                                        {
                                            "function": {
                                                "name": tc.get("function", {}).get("name", ""),
                                                "arguments": (
                                                    json.loads(tc.get("function", {}).get("arguments", "{}"))
                                                    if isinstance(tc.get("function", {}).get("arguments"), str)
                                                    else tc.get("function", {}).get("arguments", {})
                                                )
                                            }
                                        }
                                        for tc in tool_calls
                                    ]

                                if line_count <= 5:
                                    print(f"[LM Studio] Yielding chunk: {content[:50] if content else 'no content'}")
                                yield ollama_chunk
                        except json.JSONDecodeError as e:
                            print(f"[LM Studio] JSON decode error: {e}, line: {line[:100]}")
                            continue

                print(f"[LM Studio] Stream finished, total lines: {line_count}")

        except httpx.TimeoutException as e:
            print(f"[LM Studio] Timeout error: {e}")
            yield {"error": f"LM Studio timeout - model might be loading or too slow"}
        except Exception as e:
            print(f"[LM Studio] Error in chat: {type(e).__name__}: {e}")
            yield {"error": str(e)}

    async def close(self):
        """Close HTTP client"""
        await self.client.aclose()


class MCPBridgeClient:
    """Client for MCP Bridge (STDIO)"""

    def __init__(self, bridge_path: Path, server_url: str, auth_token: str):
        self.bridge_path = bridge_path
        self.server_url = server_url
        self.auth_token = auth_token

    async def call_tool(self, tool_name: str, arguments: Dict) -> Dict:
        """
        Call a tool via mcp_bridge.py

        Args:
            tool_name: Tool name
            arguments: Tool arguments

        Returns:
            Tool result
        """
        # Prepare JSON-RPC request
        request = {
            "jsonrpc": "2.0",
            "method": "tools/call",
            "params": {
                "name": tool_name,
                "arguments": arguments
            },
            "id": 1
        }

        try:
            # Call bridge via subprocess
            env = os.environ.copy()
            env.update({
                "MCP_SERVER_URL": self.server_url,
                "MCP_AUTH_TOKEN": self.auth_token
            })

            process = await asyncio.create_subprocess_exec(
                "python3",
                str(self.bridge_path),
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env
            )

            # Send request and get response
            stdout, stderr = await process.communicate(
                input=json.dumps(request).encode()
            )

            if process.returncode != 0:
                return {
                    "error": f"Bridge failed: {stderr.decode()}",
                    "success": False
                }

            # Parse response
            response = json.loads(stdout.decode())

            if "error" in response:
                return {
                    "error": response["error"].get("message", "Unknown error"),
                    "success": False
                }

            # Extract tool result from MCP response
            result = response.get("result", {})
            content = result.get("content", [])

            if content and len(content) > 0:
                text = content[0].get("text", "")
                try:
                    # Tool result is JSON string
                    data = json.loads(text)
                    return {"success": True, "data": data}
                except json.JSONDecodeError:
                    return {"success": True, "data": text}

            return {"error": "No data returned", "success": False}

        except Exception as e:
            return {"error": str(e), "success": False}

    async def list_tools(self) -> List[Dict]:
        """List available tools"""
        request = {
            "jsonrpc": "2.0",
            "method": "tools/list",
            "id": 1
        }

        try:
            env = os.environ.copy()
            env.update({
                "MCP_SERVER_URL": self.server_url,
                "MCP_AUTH_TOKEN": self.auth_token
            })

            process = await asyncio.create_subprocess_exec(
                "python3",
                str(self.bridge_path),
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env
            )

            stdout, stderr = await process.communicate(
                input=json.dumps(request).encode()
            )

            if process.returncode != 0:
                return []

            response = json.loads(stdout.decode())
            tools = response.get("result", {}).get("tools", [])
            return tools

        except Exception as e:
            print(f"Error listing tools: {e}")
            return []


# REST Endpoints

@app.get("/")
async def root():
    """Serve main page"""
    return FileResponse(Path(__file__).parent / "static" / "index.html")


@app.post("/api/models")
async def post_models(config: LLMConfig):
    """Get available models from configured LLM provider"""
    try:
        if config.provider == "lmstudio":
            client = LMStudioClient(config.url)
        else:
            client = OllamaClient(config.url)

        models = await client.list_models()
        await client.close()
        return {"models": models}
    except Exception as e:
        print(f"Error getting models: {e}")
        return {"models": []}


@app.post("/api/tools")
async def post_tools(config: MCPConfig):
    """Get available MCP tools from configured server"""
    try:
        bridge = MCPBridgeClient(
            bridge_path=MCP_BRIDGE_PATH,
            server_url=config.url,
            auth_token=config.token
        )
        tools = await bridge.list_tools()
        return {"tools": tools, "count": len(tools)}
    except Exception as e:
        print(f"Error getting tools: {e}")
        return {"tools": [], "count": 0}


@app.post("/api/health")
async def post_health(config: HealthConfig):
    """Health check with configured servers"""
    try:
        # Check LLM provider
        if config.llm_provider == "lmstudio":
            llm_client = LMStudioClient(config.llm_url)
        else:
            llm_client = OllamaClient(config.llm_url)

        llm_ok = len(await llm_client.list_models()) > 0
        await llm_client.close()

        # Check MCP server
        mcp_client = MCPBridgeClient(
            bridge_path=MCP_BRIDGE_PATH,
            server_url=config.mcp_url,
            auth_token=config.mcp_token
        )
        mcp_ok = len(await mcp_client.list_tools()) > 0

        return {
            "status": "healthy" if (llm_ok and mcp_ok) else "degraded",
            "ollama": "connected" if llm_ok else "disconnected",
            "mcp": "connected" if mcp_ok else "disconnected",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        print(f"Error in health check: {e}")
        return {
            "status": "error",
            "ollama": "disconnected",
            "mcp": "disconnected",
            "timestamp": datetime.now().isoformat()
        }


# Backwards compatibility - GET endpoints with default config
@app.get("/api/models")
async def get_models():
    """Get available models (default config)"""
    return await post_models(LLMConfig())


@app.get("/api/tools")
async def get_tools():
    """Get available tools (default config)"""
    return await post_tools(MCPConfig())


@app.get("/api/health")
async def get_health():
    """Health check (default config)"""
    return await post_health(HealthConfig())


# WebSocket Endpoint

@app.websocket("/ws/chat")
async def websocket_chat(websocket: WebSocket):
    """
    WebSocket endpoint for streaming chat

    Protocol:
    - Client sends: {"model": "...", "message": "...", "history": [...], "llm_provider": "...", "llm_url": "...", "mcp_url": "...", "mcp_token": "..."}
    - Server sends: {"type": "token", "content": "..."}
    - Server sends: {"type": "tool_call", "tool": "...", "args": {...}}
    - Server sends: {"type": "tool_result", "result": {...}}
    - Server sends: {"type": "done"}
    """
    await websocket.accept()

    try:
        while True:
            # Receive message from client
            data = await websocket.receive_json()

            model = data.get("model", "llama3.1:8b")
            message = data.get("message", "")
            history = data.get("history", [])
            temperature = data.get("temperature", 0.3)
            system_prompt = data.get("system_prompt", "")

            # Get configuration from client
            llm_provider = data.get("llm_provider", "ollama")
            llm_url = data.get("llm_url", OLLAMA_BASE_URL)
            mcp_url = data.get("mcp_url", MCP_SERVER_URL)
            mcp_token = data.get("mcp_token", MCP_AUTH_TOKEN)

            print(f"[WebSocket] Provider: {llm_provider}, Model: {model}, URL: {llm_url}")

            # Create LLM client based on provider
            if llm_provider == "lmstudio":
                llm_client = LMStudioClient(llm_url)
            else:
                llm_client = OllamaClient(llm_url)

            # Create MCP bridge client with configured server
            mcp_client = MCPBridgeClient(
                bridge_path=MCP_BRIDGE_PATH,
                server_url=mcp_url,
                auth_token=mcp_token
            )

            # Build messages with optional system prompt
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})

            messages.extend(history + [{"role": "user", "content": message}])

            # Get available tools and convert to function calling format
            mcp_tools = await mcp_client.list_tools()
            ollama_tools = []

            for tool in mcp_tools:
                # Convert MCP tool schema to function calling format
                ollama_tools.append({
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool.get("description", ""),
                        "parameters": tool.get("inputSchema", {})
                    }
                })

            # Stream response from LLM with tools
            # Note: LM Studio doesn't support function calling well, so we skip tools for it
            full_response = ""
            has_tool_call = False
            use_tools = ollama_tools if (ollama_tools and llm_provider == "ollama") else None

            if llm_provider == "lmstudio":
                print("[WebSocket] LM Studio detected - function calling disabled")

            async for chunk in llm_client.chat(
                model,
                messages,
                stream=True,
                temperature=temperature,
                tools=use_tools
            ):
                if "error" in chunk:
                    await websocket.send_json({
                        "type": "error",
                        "content": chunk["error"]
                    })
                    break

                if "message" in chunk:
                    msg = chunk["message"]
                    content = msg.get("content", "")

                    # Accumulate response
                    if content:
                        full_response += content

                    # Only stream content if we're sure it's not a tool call
                    # (tool calls starting with { won't be streamed until we verify)
                    if content and not has_tool_call:
                        # If response looks like JSON, wait until end to determine if tool call
                        if not full_response.strip().startswith("{"):
                            await websocket.send_json({
                                "type": "token",
                                "content": content
                            })

                    # Check for native tool calls (preferred)
                    tool_calls = msg.get("tool_calls")

                    # Process tool calls (native format)
                    if tool_calls:
                        has_tool_call = True
                        for tool_call in tool_calls:
                            func = tool_call.get("function", {})
                            tool_name = func.get("name")
                            tool_args = func.get("arguments", {})

                            # Notify client of tool call
                            await websocket.send_json({
                                "type": "tool_call",
                                "tool": tool_name,
                                "args": tool_args
                            })

                            # Execute tool via MCP client
                            result = await mcp_client.call_tool(tool_name, tool_args)

                            # Send result to client
                            await websocket.send_json({
                                "type": "tool_result",
                                "tool": tool_name,
                                "result": result
                            })

                            # Add tool result to conversation and continue
                            messages.append({"role": "assistant", "content": "", "tool_calls": [tool_call]})
                            messages.append({
                                "role": "tool",
                                "content": json.dumps(result)
                            })

                            # Get final response with tool result
                            async for chunk2 in llm_client.chat(
                                model,
                                messages,
                                stream=True,
                                temperature=temperature,
                                tools=ollama_tools if ollama_tools else None
                            ):
                                if "message" in chunk2:
                                    content2 = chunk2["message"].get("content", "")
                                    if content2:
                                        await websocket.send_json({
                                            "type": "token",
                                            "content": content2
                                        })

                # Check if done
                if chunk.get("done"):
                    # For models without native tool calling: parse JSON from response
                    if not has_tool_call and full_response.strip():
                        response_stripped = full_response.strip()
                        is_tool_call = False

                        # Try to parse as tool call JSON
                        if response_stripped.startswith("{") and "\"name\":" in response_stripped:
                            try:
                                tool_data = json.loads(response_stripped)
                                if "name" in tool_data and "arguments" in tool_data:
                                    is_tool_call = True
                                    # Valid tool call JSON!
                                    tool_name = tool_data["name"]
                                    tool_args = tool_data.get("arguments", {})

                                    # Notify client of tool call
                                    await websocket.send_json({
                                        "type": "tool_call",
                                        "tool": tool_name,
                                        "args": tool_args
                                    })

                                    # Execute tool
                                    result = await mcp_client.call_tool(tool_name, tool_args)

                                    # Send result to client
                                    await websocket.send_json({
                                        "type": "tool_result",
                                        "tool": tool_name,
                                        "result": result
                                    })

                                    # Add tool result to conversation and get final response
                                    messages.append({"role": "assistant", "content": full_response})
                                    messages.append({
                                        "role": "user",
                                        "content": f"Tool {tool_name} returned: {json.dumps(result)}. Please provide a natural language response based on this data."
                                    })

                                    # Get final natural language response
                                    async for chunk2 in llm_client.chat(
                                        model,
                                        messages,
                                        stream=True,
                                        temperature=temperature,
                                        tools=None  # No tools on follow-up to avoid loop
                                    ):
                                        if "message" in chunk2:
                                            content2 = chunk2["message"].get("content", "")
                                            if content2:
                                                await websocket.send_json({
                                                    "type": "token",
                                                    "content": content2
                                                })
                                        if chunk2.get("done"):
                                            break

                            except json.JSONDecodeError:
                                pass  # Not valid JSON, treat as regular text

                        # If response looked like JSON but wasn't a tool call, send it now
                        if not is_tool_call and response_stripped.startswith("{"):
                            await websocket.send_json({
                                "type": "token",
                                "content": full_response
                            })

                    await websocket.send_json({"type": "done"})

                    # Clean up LLM client
                    try:
                        await llm_client.close()
                    except:
                        pass

                    break

    except WebSocketDisconnect:
        print("Client disconnected")
    except Exception as e:
        print(f"WebSocket error: {e}")
        try:
            await websocket.send_json({
                "type": "error",
                "content": str(e)
            })
        except:
            pass


if __name__ == "__main__":
    # Check configuration
    if not MCP_AUTH_TOKEN:
        print("WARNING: MCP_AUTH_TOKEN not set!")
        print("Set with: export MCP_AUTH_TOKEN='your_token_here'")

    if not MCP_BRIDGE_PATH.exists():
        print(f"ERROR: MCP bridge not found at {MCP_BRIDGE_PATH}")
        print("Make sure mcp_bridge.py exists")
        sys.exit(1)

    print("=" * 70)
    print("NetMonitor Chat Starting")
    print("=" * 70)
    print(f"Ollama: {OLLAMA_BASE_URL}")
    print(f"MCP Server: {MCP_SERVER_URL}")
    print(f"MCP Bridge: {MCP_BRIDGE_PATH}")
    print(f"Interface: http://localhost:8000")
    print("=" * 70)

    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
